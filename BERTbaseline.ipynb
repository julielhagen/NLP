{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sent(path):\n",
    "    ents = []\n",
    "    curEnts = []\n",
    "    for line in open(path):\n",
    "        line = line.strip()\n",
    "        if line == '':\n",
    "            ents.append(curEnts)\n",
    "            curEnts = []\n",
    "        elif line[0] == '#' and len(line.split('\\t')) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            curEnts.append(line.split('\\t')[1])\n",
    "    return(ents)\n",
    "\n",
    "def read_labels(path):\n",
    "    ents = []\n",
    "    curEnts = []\n",
    "    for line in open(path):\n",
    "        line = line.strip()\n",
    "        if line == '':\n",
    "            ents.append(curEnts)\n",
    "            curEnts = []\n",
    "        elif line[0] == '#' and len(line.split('\\t')) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            curEnts.append(line.split('\\t')[2])\n",
    "    return(ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns list of lists\n",
    "training_labels = read_labels(\"baseline_data/en_ewt-ud-train.iob2\")\n",
    "training_sent = read_sent(\"baseline_data/en_ewt-ud-train.iob2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = sum(training_labels, [])\n",
    "train_sent = sum(training_sent, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_labels = read_labels(\"baseline_data/en_ewt-ud-dev.iob2\")\n",
    "dev_sent = read_sent(\"baseline_data/en_ewt-ud-dev.iob2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_labels = sum(dev_labels, [])\n",
    "dev_sent = sum(dev_sent, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = read_labels(\"baseline_data/en_ewt-ud-test.iob2\")\n",
    "test_sent = read_sent(\"baseline_data/en_ewt-ud-test.iob2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = sum(test_labels, [])\n",
    "test_sent = sum(test_sent, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A basic classifier based on the transformers (https://github.com/huggingface/transformers) \n",
    "library. It loads a masked language model (by default distilbert), and adds a linear layer for\n",
    "prediction. Example usage:\n",
    "\n",
    "python3 bert-topic.py topic-data/train.txt topic-data/dev.txt\n",
    "\"\"\"\n",
    "from typing import List, Dict\n",
    "import codecs\n",
    "import torch\n",
    "# import sys # I don't need you\n",
    "import bert.myutils as myutils # I changed this to import from bert dir\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# set seed for consistency\n",
    "torch.manual_seed(8446)\n",
    "# Set some constants\n",
    "MLM = 'bert-base-cased'\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.00001\n",
    "EPOCHS = 1\n",
    "# We have an UNK label for robustness purposes, it makes it easier to run on\n",
    "# data with other labels, or without labels.\n",
    "UNK = \"[UNK]\"\n",
    "MAX_TRAIN_SENTS=64\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class ClassModel(torch.nn.Module):\n",
    "    def __init__(self, nlabels: int, mlm: str):\n",
    "        \"\"\"\n",
    "        Model for classification with transformers.\n",
    "\n",
    "        The architecture of this model is simple, we just have a transformer\n",
    "        based language model, and add one linear layer to converts it output\n",
    "        to our prediction.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        nlabels : int\n",
    "            Vocabulary size of output space (i.e. number of labels)\n",
    "        mlm : str\n",
    "            Name of the transformers language model to use, can be found on:\n",
    "            https://huggingface.co/models\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # The transformer model to use\n",
    "        self.mlm = AutoModel.from_pretrained(mlm)\n",
    "\n",
    "        # Find the size of the output of the masked language model\n",
    "        if hasattr(self.mlm.config, 'hidden_size'):\n",
    "            self.mlm_out_size = self.mlm.config.hidden_size\n",
    "        elif hasattr(self.mlm.config, 'dim'):\n",
    "            self.mlm_out_size = self.mlm.config.dim\n",
    "        else: # if not found, guess\n",
    "            self.mlm_out_size = 768\n",
    "\n",
    "        # Create prediction layer\n",
    "        self.hidden_to_label = torch.nn.Linear(self.mlm_out_size, nlabels)\n",
    "\n",
    "    def forward(self, input: torch.tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : torch.tensor\n",
    "            Tensor with wordpiece indices. shape=(batch_size, max_sent_len).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output_scores : torch.tensor\n",
    "            ?. shape=(?,?)\n",
    "        \"\"\"\n",
    "        # Run transformer model on input\n",
    "        mlm_out = self.mlm(input)\n",
    "\n",
    "        # Keep only the last layer: shape=(batch_size, max_len, DIM_EMBEDDING)\n",
    "        mlm_out = mlm_out.last_hidden_state\n",
    "        # Keep only the output for the first ([CLS]) token: shape=(batch_size, DIM_EMBEDDING)\n",
    "        mlm_out = mlm_out[:,:1,:].squeeze()\n",
    "\n",
    "        # Matrix multiply to get scores for each label: shape=(?,?)\n",
    "        output_scores = self.hidden_to_label(mlm_out)\n",
    "\n",
    "        return output_scores\n",
    "\n",
    "    def run_eval(self, text_batched: List[torch.tensor], labels_batched: List[torch.tensor]):\n",
    "        \"\"\"\n",
    "        Run evaluation: predict and score\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        text_batched : List[torch.tensor]\n",
    "            list with batches of text, containing wordpiece indices.\n",
    "        labels_batched : List[torch.tensor]\n",
    "            list with batches of labels (converted to ints).\n",
    "        model : torch.nn.module\n",
    "            The model to use for prediction.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            accuracy of model on labels_batches given feats_batches\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        match = 0\n",
    "        total = 0\n",
    "        pred_labels_list = []\n",
    "        for sents, labels in zip(text_batched, labels_batched):\n",
    "            output_scores = self.forward(sents)\n",
    "            pred_labels = torch.argmax(output_scores, 1)\n",
    "            pred_labels_list.append(pred_labels)\n",
    "            for gold_label, pred_label in zip(labels, pred_labels):\n",
    "                total += 1\n",
    "                if gold_label.item() == pred_label.item():\n",
    "                    match+= 1\n",
    "        score = match/total\n",
    "        return score, pred_labels_list      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 2, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "{'[UNK]': 0, 1: 1, 2: 2, 3: 3}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m dev_text, dev_labels \u001b[38;5;241m=\u001b[39m dev_sent, dev_labels \u001b[38;5;66;03m# dev_text, dev_labels = myutils.read_data(sys.argv[2])\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#dev_labels = [label2id[label] for label in dev_labels]\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m dev_labels \u001b[38;5;241m=\u001b[39m [label2id\u001b[38;5;241m.\u001b[39mget(label, label2id[UNK]) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m dev_labels]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizing...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m tokzr \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MLM)\n",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m dev_text, dev_labels \u001b[38;5;241m=\u001b[39m dev_sent, dev_labels \u001b[38;5;66;03m# dev_text, dev_labels = myutils.read_data(sys.argv[2])\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#dev_labels = [label2id[label] for label in dev_labels]\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m dev_labels \u001b[38;5;241m=\u001b[39m [\u001b[43mlabel2id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel2id\u001b[49m\u001b[43m[\u001b[49m\u001b[43mUNK\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m dev_labels]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizing...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m tokzr \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MLM)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Change how we load the data to be specified as a function argument instead of a command-line argument\n",
    "train_text, train_labels = train_sent, train_labels # train_text, train_labels = myutils.read_data(sys.argv[1])\n",
    "train_text = train_text[:MAX_TRAIN_SENTS]\n",
    "train_labels = train_labels[:MAX_TRAIN_SENTS]\n",
    "\n",
    "id2label, label2id = myutils.labels2lookup(train_labels, UNK)\n",
    "NLABELS = len(id2label)\n",
    "print(train_labels)\n",
    "print(label2id)\n",
    "#train_labels = [label2id[label] for label in train_labels]\n",
    "train_labels = [label2id.get(label, label2id[UNK]) for label in train_labels]\n",
    "\n",
    "# Change how we load the data to be specified as a function argument instead of a command-line argument\n",
    "dev_text, dev_labels = dev_sent, dev_labels # dev_text, dev_labels = myutils.read_data(sys.argv[2])\n",
    "#dev_labels = [label2id[label] for label in dev_labels]\n",
    "dev_labels = [label2id.get(label, label2id[UNK]) for label in dev_labels]\n",
    "\n",
    "print('tokenizing...')\n",
    "tokzr = AutoTokenizer.from_pretrained(MLM)\n",
    "train_tokked = myutils.tok(train_text, tokzr)\n",
    "dev_tokked = myutils.tok(dev_text, tokzr)\n",
    "PAD = tokzr.pad_token_id\n",
    "\n",
    "print('converting to batches...')\n",
    "train_text_batched, train_labels_batched = myutils.to_batch(train_tokked, train_labels, BATCH_SIZE, PAD, DEVICE)\n",
    "# Note, some data is trown away if len(text_tokked)%BATCH_SIZE!= 0\n",
    "dev_text_batched, dev_labels_batched = myutils.to_batch(dev_tokked, dev_labels, BATCH_SIZE, PAD, DEVICE)\n",
    "\n",
    "print('initializing model...')\n",
    "model = ClassModel(NLABELS, MLM)\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "\n",
    "print('training...')\n",
    "for epoch in range(EPOCHS):\n",
    "    print('=====================')\n",
    "    print('starting epoch ' + str(epoch))\n",
    "    model.train() \n",
    "\n",
    "    # Loop over batches\n",
    "    loss = 0\n",
    "    for batch_idx in range(0, len(train_text_batched)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_scores = model.forward(train_text_batched[batch_idx])\n",
    "        batch_loss = loss_function(output_scores, train_labels_batched[batch_idx])\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        batch_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    dev_score = model.run_eval(dev_text_batched, dev_labels_batched)\n",
    "    print('Loss: {:.2f}'.format(loss))\n",
    "    print('Acc(dev): {:.2f}'.format(100*dev_score[0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on testing data...\n",
      "Accuracy on test data: 93.31%\n"
     ]
    }
   ],
   "source": [
    "test_text, test_labels = test_sent, test_labels # dev_text, dev_labels = myutils.read_data(sys.argv[2])\n",
    "\n",
    "\n",
    "test_labels = [label2id.get(label, label2id[UNK]) for label in test_labels]\n",
    "\n",
    "# Tokenize testing data\n",
    "test_tokked = myutils.tok(test_text, tokzr)\n",
    "\n",
    "# Convert testing data to batches\n",
    "test_text_batched, test_labels_batched = myutils.to_batch(test_tokked, test_labels, BATCH_SIZE, PAD, DEVICE)\n",
    "\n",
    "# Evaluate the model on testing data\n",
    "print('evaluating on testing data...')\n",
    "test_score = model.run_eval(test_text_batched, test_labels_batched)\n",
    "print('Accuracy on test data: {:.2f}%'.format(100 * test_score[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# Convert numerical labels back to text labels\n",
    "test_labels = [id2label[label] for label in test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '[UNK]', 1: 'O', 2: 'B-LOC', 3: 'I-LOC'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One list with predicted labels\n",
    "flatten_pred_labels = [label.item() for batch_pred_labels in test_score[1] for label in batch_pred_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_pred_labels = [id2label[label] for label in flatten_pred_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes \n",
    "indexes = []\n",
    "for id in range(len(test_labels)):\n",
    "    indexes.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_iob2_file(index_list, word_list, tag_list, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for index, word, tag in zip(index_list, word_list, tag_list):\n",
    "            f.write(f\"{index}\\t{word}\\t{tag}\\n\")\n",
    "        f.write(\"\\n\")  # Add a newline to separate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_iob2_file(indexes, test_sent , flatten_pred_labels, \"output.iob2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file name\n",
    "file_name = \"output_bert.iob2\"\n",
    "\n",
    "# Write data to the file\n",
    "with open(file_name, \"w\") as file:\n",
    "    for i, (sentence, label) in enumerate(zip(test_sent, flatten_pred_labels), start=1):\n",
    "        file.write(f\"# sent_id = answers-20070404104007AAY1Chs_ans-{str(i).zfill(4)}\\n\")\n",
    "        file.write(\"# text = \" + \" \".join(sentence) + \"\\n\")\n",
    "        for j, (token, lbl) in enumerate(zip(sentence, label), start=1):\n",
    "            file.write(f\"{j}\\t{token}\\t{lbl}\\t-\\t-\\n\")\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
