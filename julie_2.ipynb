{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/NLP/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\")  # Set NUM_LABELS according to your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_entities_from_file(file_path,type):\n",
    "    ents = []\n",
    "    curEnts = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == '':\n",
    "                ents.append(curEnts)\n",
    "                curEnts = []\n",
    "            elif line[0] == '#' and len(line.split('\\t')) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                if type == 'sent':\n",
    "                    curEnts.append(line.split('\\t')[1])\n",
    "                if type == 'label':\n",
    "                    curEnts.append(line.split('\\t')[2])\n",
    "    return ents\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'baseline_data/en_ewt-ud-train.iob2'\n",
    "sent_train = read_entities_from_file(file_path,type = 'sent')\n",
    "labels_train = read_entities_from_file(file_path,type = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where', 'in', 'the', 'world', 'is', 'Iguazu', '?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `sentences` and `labels` are lists containing sentences and their corresponding labels\n",
    "tokenized_texts_and_labels = [tokenize_and_preserve_labels(sent, labs) for sent, labs in zip(sent_train, labels_train)]\n",
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'B-LOC', 'O']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'B-LOC', 'O']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where', 'in', 'the', 'world', 'is', 'Iguazu', '?']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where', 'in', 'the', 'world', 'is', 'I', '##gua', '##zu', '?']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'myutils' has no attribute 'read_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 127\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreading data...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m     train_text, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mmyutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_data\u001b[49m(sys\u001b[38;5;241m.\u001b[39margv[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    128\u001b[0m     train_text \u001b[38;5;241m=\u001b[39m train_text[:MAX_TRAIN_SENTS]\n\u001b[1;32m    129\u001b[0m     train_labels \u001b[38;5;241m=\u001b[39m train_labels[:MAX_TRAIN_SENTS]\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'myutils' has no attribute 'read_data'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A basic classifier based on the transformers (https://github.com/huggingface/transformers) \n",
    "library. It loads a masked language model (by default distilbert), and adds a linear layer for\n",
    "prediction. Example usage:\n",
    "\n",
    "python3 bert-topic.py topic-data/train.txt topic-data/dev.txt\n",
    "\"\"\"\n",
    "from typing import List, Dict\n",
    "import codecs\n",
    "import torch\n",
    "import sys\n",
    "import myutils\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "# set seed for consistency\n",
    "torch.manual_seed(8446)\n",
    "# Set some constants\n",
    "MLM = 'distilbert-base-cased'\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.00001\n",
    "EPOCHS = 3\n",
    "# We have an UNK label for robustness purposes, it makes it easier to run on\n",
    "# data with other labels, or without labels.\n",
    "UNK = \"[UNK]\"\n",
    "MAX_TRAIN_SENTS=64\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class ClassModel(torch.nn.Module):\n",
    "    def __init__(self, nlabels: int, mlm: str):\n",
    "        \"\"\"\n",
    "        Model for classification with transformers.\n",
    "\n",
    "        The architecture of this model is simple, we just have a transformer\n",
    "        based language model, and add one linear layer to converts it output\n",
    "        to our prediction.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        nlabels : int\n",
    "            Vocabulary size of output space (i.e. number of labels)\n",
    "        mlm : str\n",
    "            Name of the transformers language model to use, can be found on:\n",
    "            https://huggingface.co/models\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # The transformer model to use\n",
    "        self.mlm = AutoModel.from_pretrained(mlm)\n",
    "\n",
    "        # Find the size of the output of the masked language model\n",
    "        if hasattr(self.mlm.config, 'hidden_size'):\n",
    "            self.mlm_out_size = self.mlm.config.hidden_size\n",
    "        elif hasattr(self.mlm.config, 'dim'):\n",
    "            self.mlm_out_size = self.mlm.config.dim\n",
    "        else: # if not found, guess\n",
    "            self.mlm_out_size = 768\n",
    "\n",
    "        # Create prediction layer\n",
    "        self.hidden_to_label = torch.nn.Linear(self.mlm_out_size, nlabels)\n",
    "\n",
    "    def forward(self, input: torch.tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : torch.tensor\n",
    "            Tensor with wordpiece indices. shape=(batch_size, max_sent_len).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output_scores : torch.tensor\n",
    "            ?. shape=(?,?)\n",
    "        \"\"\"\n",
    "        # Run transformer model on input\n",
    "        mlm_out = self.mlm(input)\n",
    "\n",
    "        # Keep only the last layer: shape=(batch_size, max_len, DIM_EMBEDDING)\n",
    "        mlm_out = mlm_out.last_hidden_state\n",
    "        # Keep only the output for the first ([CLS]) token: shape=(batch_size, DIM_EMBEDDING)\n",
    "        mlm_out = mlm_out[:,:1,:].squeeze()\n",
    "\n",
    "        # Matrix multiply to get scores for each label: shape=(?,?)\n",
    "        output_scores = self.hidden_to_label(mlm_out)\n",
    "\n",
    "        return output_scores\n",
    "\n",
    "    def run_eval(self, text_batched: List[torch.tensor], labels_batched: List[torch.tensor]):\n",
    "        \"\"\"\n",
    "        Run evaluation: predict and score\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        text_batched : List[torch.tensor]\n",
    "            list with batches of text, containing wordpiece indices.\n",
    "        labels_batched : List[torch.tensor]\n",
    "            list with batches of labels (converted to ints).\n",
    "        model : torch.nn.module\n",
    "            The model to use for prediction.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            accuracy of model on labels_batches given feats_batches\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        match = 0\n",
    "        total = 0\n",
    "        for sents, labels in zip(text_batched, labels_batched):\n",
    "            output_scores = self.forward(sents)\n",
    "            pred_labels = torch.argmax(output_scores, 1)\n",
    "            for gold_label, pred_label in zip(labels, pred_labels):\n",
    "                total += 1\n",
    "                if gold_label.item() == pred_label.item():\n",
    "                    match+= 1\n",
    "        return(match/total)        \n",
    "\n",
    "\n",
    "if len(sys.argv) < 2:\n",
    "    print('Please provide path to training and development data')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('reading data...')\n",
    "    train_text, train_labels = myutils.read_data(sys.argv[1])\n",
    "    train_text = train_text[:MAX_TRAIN_SENTS]\n",
    "    train_labels = train_labels[:MAX_TRAIN_SENTS]\n",
    "    \n",
    "    id2label, label2id = myutils.labels2lookup(train_labels, UNK)\n",
    "    NLABELS = len(id2label)\n",
    "    print(train_labels)\n",
    "    print(label2id)\n",
    "    train_labels = [label2id[label] for label in train_labels]\n",
    "    \n",
    "    dev_text, dev_labels = myutils.read_data(sys.argv[2])\n",
    "    dev_labels = [label2id[label] for label in dev_labels]\n",
    "    \n",
    "    print('tokenizing...')\n",
    "    tokzr = AutoTokenizer.from_pretrained(MLM)\n",
    "    train_tokked = myutils.tok(train_text, tokzr)\n",
    "    dev_tokked = myutils.tok(dev_text, tokzr)\n",
    "    PAD = tokzr.pad_token_id\n",
    "    \n",
    "    print('converting to batches...')\n",
    "    train_text_batched, train_labels_batched = myutils.to_batch(train_tokked, train_labels, BATCH_SIZE, PAD, DEVICE)\n",
    "    # Note, some data is trown away if len(text_tokked)%BATCH_SIZE!= 0\n",
    "    dev_text_batched, dev_labels_batched = myutils.to_batch(dev_tokked, dev_labels, BATCH_SIZE, PAD, DEVICE)\n",
    "    \n",
    "    print('initializing model...')\n",
    "    model = ClassModel(NLABELS, MLM)\n",
    "    model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    loss_function = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "    \n",
    "    print('training...')\n",
    "    for epoch in range(EPOCHS):\n",
    "        print('=====================')\n",
    "        print('starting epoch ' + str(epoch))\n",
    "        model.train() \n",
    "    \n",
    "        # Loop over batches\n",
    "        loss = 0\n",
    "        for batch_idx in range(0, len(train_text_batched)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output_scores = model.forward(train_text_batched[batch_idx])\n",
    "            batch_loss = loss_function(output_scores, train_labels_batched[batch_idx])\n",
    "            loss += batch_loss.item()\n",
    "    \n",
    "            batch_loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "    \n",
    "        dev_score = model.run_eval(dev_text_batched, dev_labels_batched)\n",
    "        print('Loss: {:.2f}'.format(loss))\n",
    "        print('Acc(dev): {:.2f}'.format(100*dev_score))\n",
    "        print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
