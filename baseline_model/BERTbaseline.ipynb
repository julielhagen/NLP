{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "import myutils\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sent(path):\n",
    "    ents = []\n",
    "    curEnts = []\n",
    "    for line in open(path):\n",
    "        line = line.strip()\n",
    "        if line == '':\n",
    "            ents.append(curEnts)\n",
    "            curEnts = []\n",
    "        elif line[0] == '#' and len(line.split('\\t')) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            curEnts.append(line.split('\\t')[1])\n",
    "    return(ents)\n",
    "\n",
    "def read_labels(path):\n",
    "    ents = []\n",
    "    curEnts = []\n",
    "    for line in open(path):\n",
    "        line = line.strip()\n",
    "        if line == '':\n",
    "            ents.append(curEnts)\n",
    "            curEnts = []\n",
    "        elif line[0] == '#' and len(line.split('\\t')) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            curEnts.append(line.split('\\t')[2])\n",
    "    return(ents)\n",
    "\n",
    "def read_index(path):\n",
    "    ents = []\n",
    "    curEnts = []\n",
    "    for line in open(path):\n",
    "        line = line.strip()\n",
    "        if line == '':\n",
    "            ents.append(curEnts)\n",
    "            curEnts = []\n",
    "        elif line[0] == '#' and len(line.split('\\t')) == 1:\n",
    "            continue\n",
    "        else:\n",
    "            curEnts.append(line.split('\\t')[0])\n",
    "    return(ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Dev and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data\n",
    "\n",
    "#returns list of lists\n",
    "training_labels = read_labels(\"baseline_data/en_ewt-ud-train.iob2\")\n",
    "training_sent = read_sent(\"baseline_data/en_ewt-ud-train.iob2\")\n",
    "\n",
    "#flatten to one list to be able to use myutils\n",
    "train_labels = sum(training_labels, [])\n",
    "train_sent = sum(training_sent, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation data\n",
    "\n",
    "dev_labels = read_labels(\"baseline_data/en_ewt-ud-dev.iob2\")\n",
    "dev_sent = read_sent(\"baseline_data/en_ewt-ud-dev.iob2\")\n",
    "\n",
    "dev_labels = sum(dev_labels, [])\n",
    "dev_sent = sum(dev_sent, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data\n",
    "#Keeping track of indeces to save to required .iob2 format for model's predictions\n",
    "\n",
    "test_labels = read_labels(\"baseline_data/en_ewt-ud-test.iob2\")\n",
    "test_sent = read_sent(\"baseline_data/en_ewt-ud-test.iob2\")\n",
    "test_index = read_index(\"baseline_data/en_ewt-ud-test.iob2\")\n",
    "\n",
    "test_labels = sum(test_labels, [])\n",
    "test_sent = sum(test_sent, [])\n",
    "test_index = sum(test_index, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A basic classifier based on the transformers (https://github.com/huggingface/transformers) \n",
    "library. It loads a masked language model (by default distilbert), and adds a linear layer for\n",
    "prediction. Example usage:\n",
    "\n",
    "python3 bert-topic.py topic-data/train.txt topic-data/dev.txt\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# set seed for consistency\n",
    "torch.manual_seed(8446)\n",
    "# Set some constants\n",
    "MLM = 'bert-large-cased'\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.00001\n",
    "EPOCHS = 3\n",
    "# We have an UNK label for robustness purposes, it makes it easier to run on\n",
    "# data with other labels, or without labels.\n",
    "UNK = \"[UNK]\"\n",
    "MAX_TRAIN_SENTS=64\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class ClassModel(torch.nn.Module):\n",
    "    def __init__(self, nlabels: int, mlm: str):\n",
    "        \"\"\"\n",
    "        Model for classification with transformers.\n",
    "\n",
    "        The architecture of this model is simple, we just have a transformer\n",
    "        based language model, and add one linear layer to converts it output\n",
    "        to our prediction.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        nlabels : int\n",
    "            Vocabulary size of output space (i.e. number of labels)\n",
    "        mlm : str\n",
    "            Name of the transformers language model to use, can be found on:\n",
    "            https://huggingface.co/models\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # The transformer model to use\n",
    "        self.mlm = AutoModel.from_pretrained(mlm)\n",
    "\n",
    "        # Find the size of the output of the masked language model\n",
    "        if hasattr(self.mlm.config, 'hidden_size'):\n",
    "            self.mlm_out_size = self.mlm.config.hidden_size\n",
    "        elif hasattr(self.mlm.config, 'dim'):\n",
    "            self.mlm_out_size = self.mlm.config.dim\n",
    "        else: # if not found, guess\n",
    "            self.mlm_out_size = 768\n",
    "\n",
    "        # Create prediction layer\n",
    "        self.hidden_to_label = torch.nn.Linear(self.mlm_out_size, nlabels)\n",
    "\n",
    "    def forward(self, input: torch.tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : torch.tensor\n",
    "            Tensor with wordpiece indices. shape=(batch_size, max_sent_len).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output_scores : torch.tensor\n",
    "            ?. shape=(?,?)\n",
    "        \"\"\"\n",
    "        # Run transformer model on input\n",
    "        mlm_out = self.mlm(input)\n",
    "\n",
    "        # Keep only the last layer: shape=(batch_size, max_len, DIM_EMBEDDING)\n",
    "        mlm_out = mlm_out.last_hidden_state\n",
    "        # Keep only the output for the first ([CLS]) token: shape=(batch_size, DIM_EMBEDDING)\n",
    "        mlm_out = mlm_out[:,:1,:].squeeze()\n",
    "\n",
    "        # Matrix multiply to get scores for each label: shape=(?,?)\n",
    "        output_scores = self.hidden_to_label(mlm_out)\n",
    "\n",
    "        return output_scores\n",
    "\n",
    "    def run_eval(self, text_batched: List[torch.tensor], labels_batched: List[torch.tensor]):\n",
    "        \"\"\"\n",
    "        Run evaluation: predict and score\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        text_batched : List[torch.tensor]\n",
    "            list with batches of text, containing wordpiece indices.\n",
    "        labels_batched : List[torch.tensor]\n",
    "            list with batches of labels (converted to ints).\n",
    "        model : torch.nn.module\n",
    "            The model to use for prediction.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            accuracy of model on labels_batches given feats_batches\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        match = 0\n",
    "        total = 0\n",
    "        pred_labels_list = []\n",
    "        for sents, labels in zip(text_batched, labels_batched):\n",
    "            output_scores = self.forward(sents)\n",
    "            pred_labels = torch.argmax(output_scores, 1)\n",
    "            pred_labels_list.append(pred_labels)\n",
    "            for gold_label, pred_label in zip(labels, pred_labels):\n",
    "                total += 1\n",
    "                if gold_label.item() == pred_label.item():\n",
    "                    match+= 1\n",
    "        score = match/total\n",
    "        return score, pred_labels_list      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "{'[UNK]': 0, 'O': 1, 'B-LOC': 2, 'I-LOC': 3}\n",
      "tokenizing...\n",
      "converting to batches...\n",
      "initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "=====================\n",
      "starting epoch 0\n",
      "Loss: 71.80\n",
      "Acc(dev): 90.21\n",
      "\n",
      "=====================\n",
      "starting epoch 1\n",
      "Loss: 32.57\n",
      "Acc(dev): 93.57\n",
      "\n",
      "=====================\n",
      "starting epoch 2\n",
      "Loss: 26.98\n",
      "Acc(dev): 94.05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_sent = train_sent[:MAX_TRAIN_SENTS]\n",
    "train_labels = train_labels[:MAX_TRAIN_SENTS]\n",
    "\n",
    "id2label, label2id = myutils.labels2lookup(train_labels, UNK)\n",
    "NLABELS = len(id2label)\n",
    "print(train_labels)\n",
    "print(label2id)\n",
    "\n",
    "#converting BIO labels to numerical labels\n",
    "train_labels = [label2id.get(label, label2id[UNK]) for label in train_labels]\n",
    "\n",
    "dev_labels = [label2id.get(label, label2id[UNK]) for label in dev_labels]\n",
    "\n",
    "print('tokenizing...')\n",
    "tokzr = AutoTokenizer.from_pretrained(MLM)\n",
    "train_tokked = myutils.tok(train_sent, tokzr)\n",
    "dev_tokked = myutils.tok(dev_sent, tokzr)\n",
    "PAD = tokzr.pad_token_id\n",
    "\n",
    "print('converting to batches...')\n",
    "train_text_batched, train_labels_batched = myutils.to_batch(train_tokked, train_labels, BATCH_SIZE, PAD, DEVICE)\n",
    "# Note, some data is thrown away if len(text_tokked)%BATCH_SIZE!= 0\n",
    "dev_text_batched, dev_labels_batched = myutils.to_batch(dev_tokked, dev_labels, BATCH_SIZE, PAD, DEVICE)\n",
    "\n",
    "print('initializing model...')\n",
    "model = ClassModel(NLABELS, MLM)\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "\n",
    "print('training...')\n",
    "for epoch in range(EPOCHS):\n",
    "    print('=====================')\n",
    "    print('starting epoch ' + str(epoch))\n",
    "    model.train() \n",
    "\n",
    "    # Loop over batches\n",
    "    loss = 0\n",
    "    for batch_idx in range(0, len(train_text_batched)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_scores = model.forward(train_text_batched[batch_idx])\n",
    "        batch_loss = loss_function(output_scores, train_labels_batched[batch_idx])\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        batch_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    dev_score = model.run_eval(dev_text_batched, dev_labels_batched)\n",
    "    print('Loss: {:.2f}'.format(loss))\n",
    "    print('Acc(dev): {:.2f}'.format(100*dev_score[0]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on testing data...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#Convert BIO labels to numerical labels\n",
    "test_labels = [label2id.get(label, label2id[UNK]) for label in test_labels]\n",
    "\n",
    "# Tokenize testing data\n",
    "test_tokked = myutils.tok(test_sent, tokzr)\n",
    "\n",
    "# Convert testing data to batches\n",
    "test_text_batched, test_labels_batched = myutils.to_batch(test_tokked, test_labels, 8, PAD, DEVICE)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "print('evaluating on testing data...')\n",
    "test_score = model.run_eval(test_text_batched, test_labels_batched)\n",
    "print('Accuracy on test data: {:.2f}%'.format(100 * test_score[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to .iob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# One list with predicted labels\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m [label\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m batch_pred_labels \u001b[38;5;129;01min\u001b[39;00m test_score[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m batch_pred_labels]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#we define id2label earlier\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#id2label = {v: k for k, v in label2id.items()}\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Convert numerical labels back to text labels\u001b[39;00m\n\u001b[1;32m      8\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m [id2label[\u001b[38;5;28mint\u001b[39m(label)] \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m pred_labels]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_score' is not defined"
     ]
    }
   ],
   "source": [
    "# One list with predicted labels\n",
    "pred_labels = [label.item() for batch_pred_labels in test_score[1] for label in batch_pred_labels]\n",
    "\n",
    "#we define id2label earlier\n",
    "#id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# Convert numerical labels back to text labels\n",
    "pred_labels = [id2label[int(label)] for label in pred_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_iob2_file(index_list, word_list, tag_list, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for index, word, tag in zip(index_list, word_list, tag_list):\n",
    "            f.write(f\"{index}\\t{word}\\t{tag}\\n\")\n",
    "        f.write(\"\\n\")  # Add a newline to separate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_iob2_file(test_index, test_sent, pred_labels, 'wrong_format.iob2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above turned out to be wrong format, so we had to change it - adding new line separator between sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iob2_file(file_path):\n",
    "    sentences = []\n",
    "    with open(file_path, \"r\") as file:\n",
    "        new_sentence = []\n",
    "        for line in file:\n",
    "            line = line.split()\n",
    "            if line:  # Check if the line is not empty\n",
    "                if int(line[0]) == 1:\n",
    "                    if new_sentence:  # Append the previous sentence if it's not empty\n",
    "                        sentences.append(new_sentence)\n",
    "                    new_sentence = [line]  # Start a new sentence\n",
    "                else:\n",
    "                    new_sentence.append(line)\n",
    "        if new_sentence:  # Append the last sentence if it's not empty\n",
    "            sentences.append(new_sentence)\n",
    "    return sentences\n",
    "\n",
    "sentences = read_iob2_file(\"wrong_format.iob2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_iob2_format(sentences, output_file):\n",
    "    with open(output_file, \"w\") as file:\n",
    "        for sentence in sentences:\n",
    "            for token in sentence:\n",
    "                file.write(\"\\t\".join(token) + \"\\n\")\n",
    "            file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_iob2_format(sentences, \"predictions.iob2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
